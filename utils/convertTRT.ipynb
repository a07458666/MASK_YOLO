{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/insign/Doc/insign/Python_utils/torch2trt')\n",
    "from torch2trt import torch2trt\n",
    "from torch2trt import TRTModule\n",
    "import torch\n",
    "import datetime, time, copy, yaml\n",
    "device = torch.device('cuda')\n",
    "from copy import deepcopy\n",
    "import sys, os, time\n",
    "sys.path.append('/home/insign/Doc/insign/Mask_yolo')\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np \n",
    "from model.od.data.datasets import letterbox\n",
    "from typing import Any\n",
    "from model.backbone_YOLO import *\n",
    "from model.head_RCNN import *\n",
    "from model.groundtrue_import import *\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "def image_loading(img_path):\n",
    "    image = cv2.imread(str(img_path))\n",
    "    img_h, img_w = image.shape[0], image.shape[1]\n",
    "    image = letterbox(image, new_shape=640)[0]\n",
    "    im0s = deepcopy(image)\n",
    "    image = image[:, :, ::-1].transpose(2, 0, 1)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    image = torch.from_numpy(image).unsqueeze(0).to(device)\n",
    "    image = image.float()\n",
    "    image /= 255.0\n",
    "    return image, im0s, img_h, img_w\n",
    "\n",
    "image, im0s, img_h, img_w = image_loading('/home/insign/Doc/insign/Mask_yolo/Polyp/Images/2022012001_33.jpg')\n",
    "\n",
    "yolo_cfg = '/home/insign/Doc/insign/Mask_yolo/config/config.yaml'\n",
    "with open(yolo_cfg, 'r') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.Loader)\n",
    "device = torch.device('cuda')\n",
    "yolo = model_manipulate(cfg['model']['weight']).eval().to(device)\n",
    "x = image\n",
    "pred = yolo(x)\n",
    "rois = non_max_suppression(pred['rois'][0],cfg['nms']['conf_thres'], cfg['nms']['iou_thres'], classes= cfg['nms']['classes'],agnostic=cfg['nms']['agnostic_nms'])\n",
    "boxes = rois[0][:,:4]\n",
    "feature_map = pred['feature_map']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torch import nn\n",
    "\n",
    "class ROIAlign(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.featurealign1 = nn.Conv2d(256, 512, 1, 1).to('cuda')\n",
    "        self.featurealign2 = nn.Conv2d(512, 512, 1, 1).to('cuda')\n",
    "        self.featurealign3 = nn.Conv2d(1024, 512, 1, 1).to('cuda')\n",
    "        self.mask_roi_pool = MultiScaleRoIAlign(featmap_names=[\"feat1\",\"feat2\",\"feat3\"] ,output_size=28, sampling_ratio=2)\n",
    "        \n",
    "    def featuremapPack(self, feat1, feat2, feat3):\n",
    "        from collections import OrderedDict\n",
    "        map = {}\n",
    "        key_name = [\"feat1\",\"feat2\",\"feat3\"]\n",
    "        feature_map = [feat1,feat2,feat3]\n",
    "        for i,j in zip(key_name, feature_map):\n",
    "            map[i] = j\n",
    "        return map\n",
    "    \n",
    "    def forward(self, feat1, feat2, feat3, box):\n",
    "        features_0 = self.featurealign1(feat1)\n",
    "        features_1 = self.featurealign2(feat2)\n",
    "        features_2 = self.featurealign3(feat3)\n",
    "        featuremap = self.featuremapPack(features_0, features_1, features_2)\n",
    "        # featurepooling = self.mask_roi_pool(featuremap, [box], image_shapes=[(640,640)])\n",
    "        return features_2\n",
    "\n",
    "model = ROIAlign()\n",
    "# featuremap = model(feature_map[0], feature_map[1], feature_map[2], boxes)\n",
    "model_trt = torch2trt(model, [feature_map[0], feature_map[1], feature_map[2], boxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.10804]]) *******************\n",
      "tensor([[0.10804]]) **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TensorRT] ERROR: Tensor: output_0 trying to set to TensorLocation::kHOST but only kDEVICE is supported (only network inputs may be on host)\n",
      "[TensorRT] ERROR: Tensor: input_0 set to TensorLocation::kHOST but only kDEVICE is supported (only RNNv2 allows host input)\n",
      "[TensorRT] ERROR: Network validation failed.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/insign/Doc/insign/Mask_yolo/utils/convertTRT.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/insign/Doc/insign/Mask_yolo/utils/convertTRT.ipynb#ch0000005?line=11'>12</a>\u001b[0m y \u001b[39m=\u001b[39m clamp(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/insign/Doc/insign/Mask_yolo/utils/convertTRT.ipynb#ch0000005?line=12'>13</a>\u001b[0m model_trt \u001b[39m=\u001b[39m torch2trt(clamp,[x])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/insign/Doc/insign/Mask_yolo/utils/convertTRT.ipynb#ch0000005?line=13'>14</a>\u001b[0m trt \u001b[39m=\u001b[39m model_trt(\u001b[39m-\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/insign/Doc/insign/Mask_yolo/utils/convertTRT.ipynb#ch0000005?line=14'>15</a>\u001b[0m y \u001b[39m=\u001b[39m trt(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/insign/Doc/insign/Mask_yolo/utils/convertTRT.ipynb#ch0000005?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(y)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Doc/insign/Python_utils/torch2trt/torch2trt/torch2trt.py:459\u001b[0m, in \u001b[0;36mTRTModule.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs):\n\u001b[0;32m--> 459\u001b[0m     batch_size \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    460\u001b[0m     bindings \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_names) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_names))\n\u001b[1;32m    462\u001b[0m     \u001b[39m# create output tensors\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "class test(nn.Module):\n",
    "    def __init__(self, min, max):\n",
    "        super().__init__()\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "    def forward(self, x):\n",
    "        y=torch.clamp(x, self.min, self.max)\n",
    "        return y\n",
    "\n",
    "x = torch.rand(1,1)\n",
    "clamp = test(min=-0.5, max=0.5)\n",
    "y = clamp(x)\n",
    "model_trt = torch2trt(clamp,[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
